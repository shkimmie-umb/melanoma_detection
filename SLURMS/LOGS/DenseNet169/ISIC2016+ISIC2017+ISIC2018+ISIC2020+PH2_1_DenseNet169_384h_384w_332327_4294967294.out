Sat 04 May 2024 07:30:19 PM EDT
Python 3.9.7
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
My SLURM_ARRAY_TASK_ID: 
DB: ['ISIC2016', 'ISIC2017', 'ISIC2018', 'ISIC2020', 'PH2']
IMG_SIZE: [384, 384]
CLASSIFIER: DenseNet169
JOB_INDEX: None
Start training augmented images
Combining...
Combining 1th db out of 5 dbs
Combining 2th db out of 5 dbs
Combining 3th db out of 5 dbs
Combining 4th db out of 5 dbs
Combining 5th db out of 5 dbs
Stacking data
Combining complete
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
densenet169 (Functional)     (None, 1664)              12642880  
_________________________________________________________________
dense (Dense)                (None, 512)               852480    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
batch_normalization (BatchNo (None, 512)               2048      
_________________________________________________________________
dense_1 (Dense)              (None, 256)               131328    
_________________________________________________________________
batch_normalization_1 (Batch (None, 256)               1024      
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 514       
=================================================================
Total params: 13,630,274
Trainable params: 985,858
Non-trainable params: 12,644,416
_________________________________________________________________
Fitting ISIC2016+ISIC2017+ISIC2018+ISIC2020+PH2_aug_DenseNet169_384h_384w_None model...
model_name: ISIC2016+ISIC2017+ISIC2018+ISIC2020+PH2_aug_DenseNet169_384h_384w_None
Epoch: 1 loss: 0.7794 accuracy: 0.8365 val_loss: 0.5016 val_accuracy: 0.9471
Epoch: 2 loss: 0.5592 accuracy: 0.8999 val_loss: 0.4299 val_accuracy: 0.9572
Epoch: 3 loss: 0.4895 accuracy: 0.9182 val_loss: 0.4229 val_accuracy: 0.9492
Epoch: 4 loss: 0.4467 accuracy: 0.9273 val_loss: 0.3820 val_accuracy: 0.9568
Epoch: 5 loss: 0.4134 accuracy: 0.9320 val_loss: 0.3616 val_accuracy: 0.9578
Epoch: 6 loss: 0.3838 accuracy: 0.9390 val_loss: 0.3658 val_accuracy: 0.9470
Epoch: 7 loss: 0.3632 accuracy: 0.9406 val_loss: 0.3287 val_accuracy: 0.9573
Epoch: 8 loss: 0.3393 accuracy: 0.9447 val_loss: 0.3004 val_accuracy: 0.9628
Epoch: 9 loss: 0.3150 accuracy: 0.9500 val_loss: 0.2942 val_accuracy: 0.9611
Epoch: 10 loss: 0.2984 accuracy: 0.9521 val_loss: 0.2928 val_accuracy: 0.9578
Epoch: 11 loss: 0.2880 accuracy: 0.9526 val_loss: 0.2730 val_accuracy: 0.9621
Epoch: 12 loss: 0.2712 accuracy: 0.9546 val_loss: 0.2551 val_accuracy: 0.9684
Epoch: 13 loss: 0.2602 accuracy: 0.9565 val_loss: 0.2556 val_accuracy: 0.9618
Epoch: 14 loss: 0.2476 accuracy: 0.9581 val_loss: 0.2718 val_accuracy: 0.9537
Epoch: 15 loss: 0.2325 accuracy: 0.9623 val_loss: 0.2566 val_accuracy: 0.9547
Epoch: 16 loss: 0.2231 accuracy: 0.9626 val_loss: 0.2316 val_accuracy: 0.9667
Epoch: 17 loss: 0.2109 accuracy: 0.9646 val_loss: 0.2424 val_accuracy: 0.9596
Epoch: 18 loss: 0.2065 accuracy: 0.9645 val_loss: 0.2217 val_accuracy: 0.9670
Epoch: 19 loss: 0.1986 accuracy: 0.9652 val_loss: 0.2262 val_accuracy: 0.9625
Epoch: 20 loss: 0.1892 accuracy: 0.9669 val_loss: 0.2146 val_accuracy: 0.9627
Epoch: 21 loss: 0.1893 accuracy: 0.9664 val_loss: 0.2099 val_accuracy: 0.9635
Epoch: 22 loss: 0.1810 accuracy: 0.9679 val_loss: 0.2079 val_accuracy: 0.9653
Epoch: 23 loss: 0.1730 accuracy: 0.9701 val_loss: 0.2131 val_accuracy: 0.9620
Epoch: 24 loss: 0.1698 accuracy: 0.9687 val_loss: 0.2037 val_accuracy: 0.9719
Epoch: 25 loss: 0.1668 accuracy: 0.9693 val_loss: 0.1994 val_accuracy: 0.9695
Epoch: 26 loss: 0.1632 accuracy: 0.9705 val_loss: 0.2201 val_accuracy: 0.9583
Epoch: 27 loss: 0.1570 accuracy: 0.9712 val_loss: 0.2077 val_accuracy: 0.9603
Epoch: 28 loss: 0.1520 accuracy: 0.9720 val_loss: 0.1928 val_accuracy: 0.9641
Epoch: 29 loss: 0.1456 accuracy: 0.9740 val_loss: 0.1892 val_accuracy: 0.9666
Epoch: 30 loss: 0.1528 accuracy: 0.9703 val_loss: 0.1888 val_accuracy: 0.9646
Epoch: 31 loss: 0.1433 accuracy: 0.9740 val_loss: 0.1885 val_accuracy: 0.9654
Epoch: 32 loss: 0.1427 accuracy: 0.9730 val_loss: 0.1861 val_accuracy: 0.9631
Epoch: 33 loss: 0.1435 accuracy: 0.9722 val_loss: 0.2027 val_accuracy: 0.9596
Epoch: 34 loss: 0.1437 accuracy: 0.9723 val_loss: 0.1813 val_accuracy: 0.9650
Epoch: 35 loss: 0.1347 accuracy: 0.9745 val_loss: 0.2016 val_accuracy: 0.9571
Epoch: 36 loss: 0.1312 accuracy: 0.9757 val_loss: 0.1793 val_accuracy: 0.9680
Epoch: 37 loss: 0.1323 accuracy: 0.9748 val_loss: 0.1776 val_accuracy: 0.9703
Epoch: 38 loss: 0.1256 accuracy: 0.9768 val_loss: 0.1891 val_accuracy: 0.9667
Epoch: 39 loss: 0.1223 accuracy: 0.9775 val_loss: 0.1878 val_accuracy: 0.9670
Epoch: 40 loss: 0.1201 accuracy: 0.9776 val_loss: 0.1822 val_accuracy: 0.9664
Epoch: 41 loss: 0.1245 accuracy: 0.9754 val_loss: 0.1672 val_accuracy: 0.9702
Epoch: 42 loss: 0.1202 accuracy: 0.9767 val_loss: 0.1803 val_accuracy: 0.9706
Epoch: 43 loss: 0.1173 accuracy: 0.9781 val_loss: 0.1943 val_accuracy: 0.9604
Epoch: 44 loss: 0.1137 accuracy: 0.9791 val_loss: 0.1773 val_accuracy: 0.9677
Epoch: 45 loss: 0.1156 accuracy: 0.9781 val_loss: 0.2037 val_accuracy: 0.9565
Epoch: 46 loss: 0.1140 accuracy: 0.9786 val_loss: 0.1834 val_accuracy: 0.9668

Epoch 00046: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.
Epoch: 47 loss: 0.1017 accuracy: 0.9831 val_loss: 0.1728 val_accuracy: 0.9675
Epoch: 48 loss: 0.1023 accuracy: 0.9821 val_loss: 0.1891 val_accuracy: 0.9624
Epoch: 49 loss: 0.0990 accuracy: 0.9825 val_loss: 0.1870 val_accuracy: 0.9615
Epoch: 50 loss: 0.0994 accuracy: 0.9832 val_loss: 0.1918 val_accuracy: 0.9703
Epoch: 51 loss: 0.0977 accuracy: 0.9835 val_loss: 0.1732 val_accuracy: 0.9653

Epoch 00051: ReduceLROnPlateau reducing learning rate to 6.399999838322402e-05.
End of augmented training
Finish
Job ended!
