Fri 03 May 2024 05:11:31 AM EDT
Python 3.9.7
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
My SLURM_ARRAY_TASK_ID: 
DB: ['ISIC2016', 'PAD_UFES_20', 'MEDNODE', 'KaggleMB']
IMG_SIZE: [384, 384]
CLASSIFIER: EfficientNetB1
JOB_INDEX: None
Start training augmented images
Combining...
Combining 1th db out of 4 dbs
Combining 2th db out of 4 dbs
Combining 3th db out of 4 dbs
Combining 4th db out of 4 dbs
Stacking data
Combining complete
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
efficientnetb1 (Functional)  (None, 1280)              6575239   
_________________________________________________________________
dense (Dense)                (None, 512)               655872    
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
batch_normalization (BatchNo (None, 512)               2048      
_________________________________________________________________
dense_1 (Dense)              (None, 256)               131328    
_________________________________________________________________
batch_normalization_1 (Batch (None, 256)               1024      
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 514       
=================================================================
Total params: 7,366,025
Trainable params: 789,250
Non-trainable params: 6,576,775
_________________________________________________________________
Fitting ISIC2016+PAD_UFES_20+MEDNODE+KaggleMB_aug_EfficientNetB1_384h_384w_None model...
model_name: ISIC2016+PAD_UFES_20+MEDNODE+KaggleMB_aug_EfficientNetB1_384h_384w_None
Epoch: 1 loss: 1.4036 accuracy: 0.5147 val_loss: 1.2314 val_accuracy: 0.2379
Epoch: 2 loss: 1.2769 accuracy: 0.5108 val_loss: 1.2475 val_accuracy: 0.2379
Epoch: 3 loss: 1.2104 accuracy: 0.5318 val_loss: 1.3167 val_accuracy: 0.2379
Epoch: 4 loss: 1.1822 accuracy: 0.5292 val_loss: 1.0894 val_accuracy: 0.7621
Epoch: 5 loss: 1.1638 accuracy: 0.5308 val_loss: 1.1372 val_accuracy: 0.2379
Epoch: 6 loss: 1.1407 accuracy: 0.5337 val_loss: 1.0746 val_accuracy: 0.7621
Epoch: 7 loss: 1.1254 accuracy: 0.5427 val_loss: 1.0401 val_accuracy: 0.7621
Epoch: 8 loss: 1.1214 accuracy: 0.5231 val_loss: 1.0774 val_accuracy: 0.7621
Epoch: 9 loss: 1.1018 accuracy: 0.5430 val_loss: 1.0861 val_accuracy: 0.2845
Epoch: 10 loss: 1.0904 accuracy: 0.5457 val_loss: 1.1769 val_accuracy: 0.2379
Epoch: 11 loss: 1.0792 accuracy: 0.5471 val_loss: 1.0077 val_accuracy: 0.7621
Epoch: 12 loss: 1.0691 accuracy: 0.5534 val_loss: 1.0144 val_accuracy: 0.7621
Epoch: 13 loss: 1.0645 accuracy: 0.5565 val_loss: 0.9952 val_accuracy: 0.7621
Epoch: 14 loss: 1.0590 accuracy: 0.5411 val_loss: 0.9625 val_accuracy: 0.7621
Epoch: 15 loss: 1.0503 accuracy: 0.5503 val_loss: 0.9998 val_accuracy: 0.7621
Epoch: 16 loss: 1.0438 accuracy: 0.5475 val_loss: 0.9597 val_accuracy: 0.7621
Epoch: 17 loss: 1.0325 accuracy: 0.5569 val_loss: 0.9415 val_accuracy: 0.7621
Epoch: 18 loss: 1.0218 accuracy: 0.5584 val_loss: 0.9750 val_accuracy: 0.7621
Epoch: 19 loss: 1.0131 accuracy: 0.5620 val_loss: 0.9923 val_accuracy: 0.7621
Epoch: 20 loss: 1.0080 accuracy: 0.5620 val_loss: 1.0631 val_accuracy: 0.2379
Epoch: 21 loss: 0.9996 accuracy: 0.5674 val_loss: 0.9370 val_accuracy: 0.7621
Epoch: 22 loss: 0.9948 accuracy: 0.5549 val_loss: 0.9415 val_accuracy: 0.7621
Epoch: 23 loss: 0.9882 accuracy: 0.5573 val_loss: 0.9261 val_accuracy: 0.7621
Epoch: 24 loss: 0.9793 accuracy: 0.5580 val_loss: 0.9533 val_accuracy: 0.7621
Epoch: 25 loss: 0.9741 accuracy: 0.5546 val_loss: 0.9947 val_accuracy: 0.2379
Epoch: 26 loss: 0.9627 accuracy: 0.5642 val_loss: 0.8857 val_accuracy: 0.7621
Epoch: 27 loss: 0.9582 accuracy: 0.5666 val_loss: 0.9285 val_accuracy: 0.7621
Epoch: 28 loss: 0.9495 accuracy: 0.5682 val_loss: 0.8758 val_accuracy: 0.7621
Epoch: 29 loss: 0.9404 accuracy: 0.5690 val_loss: 0.8622 val_accuracy: 0.7621
Epoch: 30 loss: 0.9335 accuracy: 0.5737 val_loss: 0.8854 val_accuracy: 0.7621
Epoch: 31 loss: 0.9286 accuracy: 0.5666 val_loss: 0.8009 val_accuracy: 0.7621
Epoch: 32 loss: 0.9250 accuracy: 0.5602 val_loss: 0.8715 val_accuracy: 0.7621
Epoch: 33 loss: 0.9146 accuracy: 0.5680 val_loss: 0.8448 val_accuracy: 0.7621
Epoch: 34 loss: 0.9130 accuracy: 0.5541 val_loss: 0.8909 val_accuracy: 0.7621
Epoch: 35 loss: 0.9010 accuracy: 0.5650 val_loss: 0.8486 val_accuracy: 0.7621
Epoch: 36 loss: 0.8922 accuracy: 0.5731 val_loss: 0.7985 val_accuracy: 0.7621
Epoch: 37 loss: 0.8923 accuracy: 0.5597 val_loss: 0.8286 val_accuracy: 0.7621
Epoch: 38 loss: 0.8805 accuracy: 0.5709 val_loss: 0.8338 val_accuracy: 0.7621
Epoch: 39 loss: 0.8790 accuracy: 0.5598 val_loss: 0.8792 val_accuracy: 0.7196
Epoch: 40 loss: 0.8713 accuracy: 0.5692 val_loss: 0.7601 val_accuracy: 0.7621
Epoch: 41 loss: 0.8679 accuracy: 0.5612 val_loss: 0.8096 val_accuracy: 0.7621
Epoch: 42 loss: 0.8557 accuracy: 0.5724 val_loss: 0.8060 val_accuracy: 0.7621
Epoch: 43 loss: 0.8546 accuracy: 0.5693 val_loss: 0.8062 val_accuracy: 0.7621
Epoch: 44 loss: 0.8444 accuracy: 0.5805 val_loss: 0.7520 val_accuracy: 0.7621
Epoch: 45 loss: 0.8436 accuracy: 0.5657 val_loss: 0.7579 val_accuracy: 0.7621
Epoch: 46 loss: 0.8384 accuracy: 0.5677 val_loss: 0.7359 val_accuracy: 0.7621
Epoch: 47 loss: 0.8361 accuracy: 0.5549 val_loss: 0.7819 val_accuracy: 0.7621
Epoch: 48 loss: 0.8282 accuracy: 0.5590 val_loss: 0.7599 val_accuracy: 0.7621
Epoch: 49 loss: 0.8227 accuracy: 0.5585 val_loss: 0.8401 val_accuracy: 0.2379
Epoch: 50 loss: 0.8181 accuracy: 0.5589 val_loss: 0.7552 val_accuracy: 0.7621
Epoch: 51 loss: 0.8119 accuracy: 0.5640 val_loss: 0.7625 val_accuracy: 0.7621

Epoch 00051: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.
Epoch: 52 loss: 0.8074 accuracy: 0.5712 val_loss: 0.7591 val_accuracy: 0.7621
Epoch: 53 loss: 0.8067 accuracy: 0.5672 val_loss: 0.7733 val_accuracy: 0.7621
Epoch: 54 loss: 0.8011 accuracy: 0.5664 val_loss: 0.7127 val_accuracy: 0.7621
Epoch: 55 loss: 0.7924 accuracy: 0.5844 val_loss: 0.7465 val_accuracy: 0.7621
Epoch: 56 loss: 0.7944 accuracy: 0.5633 val_loss: 0.7073 val_accuracy: 0.7621
Epoch: 57 loss: 0.7916 accuracy: 0.5713 val_loss: 0.7315 val_accuracy: 0.7621
Epoch: 58 loss: 0.7888 accuracy: 0.5652 val_loss: 0.7052 val_accuracy: 0.7621
Epoch: 59 loss: 0.7832 accuracy: 0.5694 val_loss: 0.7420 val_accuracy: 0.7621
Epoch: 60 loss: 0.7804 accuracy: 0.5718 val_loss: 0.7169 val_accuracy: 0.7621
Epoch: 61 loss: 0.7754 accuracy: 0.5759 val_loss: 0.7041 val_accuracy: 0.7621
Epoch: 62 loss: 0.7705 accuracy: 0.5841 val_loss: 0.6732 val_accuracy: 0.7621
Epoch: 63 loss: 0.7731 accuracy: 0.5694 val_loss: 0.6895 val_accuracy: 0.7621
Epoch: 64 loss: 0.7645 accuracy: 0.5863 val_loss: 0.6687 val_accuracy: 0.7621
Epoch: 65 loss: 0.7694 accuracy: 0.5702 val_loss: 0.7121 val_accuracy: 0.7621
Epoch: 66 loss: 0.7637 accuracy: 0.5698 val_loss: 0.6543 val_accuracy: 0.7621
Epoch: 67 loss: 0.7657 accuracy: 0.5598 val_loss: 0.7199 val_accuracy: 0.7621
Epoch: 68 loss: 0.7618 accuracy: 0.5628 val_loss: 0.7141 val_accuracy: 0.7621
Epoch: 69 loss: 0.7580 accuracy: 0.5708 val_loss: 0.6865 val_accuracy: 0.7621
Epoch: 70 loss: 0.7555 accuracy: 0.5696 val_loss: 0.7058 val_accuracy: 0.7621
Epoch: 71 loss: 0.7572 accuracy: 0.5601 val_loss: 0.6864 val_accuracy: 0.7621

Epoch 00071: ReduceLROnPlateau reducing learning rate to 6.399999838322402e-05.
Epoch: 72 loss: 0.7499 accuracy: 0.5685 val_loss: 0.6948 val_accuracy: 0.7621
Epoch: 73 loss: 0.7474 accuracy: 0.5799 val_loss: 0.6865 val_accuracy: 0.7621
Epoch: 74 loss: 0.7478 accuracy: 0.5700 val_loss: 0.6754 val_accuracy: 0.7621
Epoch: 75 loss: 0.7490 accuracy: 0.5641 val_loss: 0.6657 val_accuracy: 0.7621
Epoch: 76 loss: 0.7447 accuracy: 0.5729 val_loss: 0.6811 val_accuracy: 0.7621

Epoch 00076: ReduceLROnPlateau reducing learning rate to 5.119999987073243e-05.
End of augmented training
Finish
Job ended!
