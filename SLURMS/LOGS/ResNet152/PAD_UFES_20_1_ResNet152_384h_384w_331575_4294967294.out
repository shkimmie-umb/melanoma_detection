Thu 02 May 2024 03:03:18 PM EDT
Python 3.9.7
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
My SLURM_ARRAY_TASK_ID: 
DB: ['PAD_UFES_20']
IMG_SIZE: [384, 384]
CLASSIFIER: ResNet152
JOB_INDEX: None
Start training augmented images
Combining...
Combining 1th db out of 1 dbs
Stacking data
Combining complete
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
resnet152 (Functional)       (None, 2048)              58370944  
_________________________________________________________________
dense (Dense)                (None, 512)               1049088   
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
batch_normalization (BatchNo (None, 512)               2048      
_________________________________________________________________
dense_1 (Dense)              (None, 256)               131328    
_________________________________________________________________
batch_normalization_1 (Batch (None, 256)               1024      
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 514       
=================================================================
Total params: 59,554,946
Trainable params: 1,182,466
Non-trainable params: 58,372,480
_________________________________________________________________
Fitting PAD_UFES_20_aug_ResNet152_384h_384w_None model...
model_name: PAD_UFES_20_aug_ResNet152_384h_384w_None
Epoch: 1 loss: 1.5762 accuracy: 0.5156 val_loss: 1.2034 val_accuracy: 0.0891
Epoch: 2 loss: 1.3365 accuracy: 0.5473 val_loss: 1.3261 val_accuracy: 0.0239
Epoch: 3 loss: 1.1503 accuracy: 0.5813 val_loss: 1.2209 val_accuracy: 0.0239
Epoch: 4 loss: 1.0611 accuracy: 0.5992 val_loss: 0.9539 val_accuracy: 0.9739
Epoch: 5 loss: 0.9871 accuracy: 0.6180 val_loss: 0.6687 val_accuracy: 0.9761
Epoch: 6 loss: 0.9167 accuracy: 0.6363 val_loss: 0.5971 val_accuracy: 0.9761
Epoch: 7 loss: 0.8782 accuracy: 0.6574 val_loss: 0.5830 val_accuracy: 0.9761
Epoch: 8 loss: 0.8464 accuracy: 0.6695 val_loss: 0.3797 val_accuracy: 0.9761
Epoch: 9 loss: 0.7927 accuracy: 0.6949 val_loss: 0.4469 val_accuracy: 0.9761
Epoch: 10 loss: 0.7825 accuracy: 0.7211 val_loss: 0.4906 val_accuracy: 0.9761
Epoch: 11 loss: 0.7503 accuracy: 0.7230 val_loss: 0.3865 val_accuracy: 0.9761
Epoch: 12 loss: 0.7427 accuracy: 0.7262 val_loss: 0.4311 val_accuracy: 0.9761
Epoch: 13 loss: 0.7102 accuracy: 0.7516 val_loss: 0.2741 val_accuracy: 0.9761
Epoch: 14 loss: 0.7034 accuracy: 0.7547 val_loss: 0.2674 val_accuracy: 0.9761
Epoch: 15 loss: 0.6948 accuracy: 0.7629 val_loss: 0.2727 val_accuracy: 0.9761
Epoch: 16 loss: 0.6862 accuracy: 0.7676 val_loss: 0.2615 val_accuracy: 0.9761
Epoch: 17 loss: 0.6548 accuracy: 0.7855 val_loss: 0.2571 val_accuracy: 0.9761
Epoch: 18 loss: 0.6856 accuracy: 0.7707 val_loss: 0.2760 val_accuracy: 0.9761
Epoch: 19 loss: 0.6636 accuracy: 0.7836 val_loss: 0.2529 val_accuracy: 0.9761
Epoch: 20 loss: 0.6591 accuracy: 0.7937 val_loss: 0.2485 val_accuracy: 0.9761
Epoch: 21 loss: 0.6272 accuracy: 0.8125 val_loss: 0.2563 val_accuracy: 0.9761
Epoch: 22 loss: 0.6411 accuracy: 0.8008 val_loss: 0.2748 val_accuracy: 0.9761
Epoch: 23 loss: 0.6189 accuracy: 0.8152 val_loss: 0.2543 val_accuracy: 0.9761
Epoch: 24 loss: 0.6359 accuracy: 0.8102 val_loss: 0.2817 val_accuracy: 0.9761
Epoch: 25 loss: 0.6325 accuracy: 0.8102 val_loss: 0.2582 val_accuracy: 0.9761

Epoch 00025: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.
Epoch: 26 loss: 0.6154 accuracy: 0.8168 val_loss: 0.2407 val_accuracy: 0.9761
Epoch: 27 loss: 0.6256 accuracy: 0.8059 val_loss: 0.2395 val_accuracy: 0.9761
Epoch: 28 loss: 0.6274 accuracy: 0.8129 val_loss: 0.2398 val_accuracy: 0.9761
Epoch: 29 loss: 0.6141 accuracy: 0.8207 val_loss: 0.2388 val_accuracy: 0.9761
Epoch: 30 loss: 0.6027 accuracy: 0.8199 val_loss: 0.2386 val_accuracy: 0.9761
Epoch: 31 loss: 0.6176 accuracy: 0.8238 val_loss: 0.2520 val_accuracy: 0.9761
Epoch: 32 loss: 0.6186 accuracy: 0.8168 val_loss: 0.2368 val_accuracy: 0.9761
Epoch: 33 loss: 0.5999 accuracy: 0.8273 val_loss: 0.2352 val_accuracy: 0.9761
Epoch: 34 loss: 0.6028 accuracy: 0.8191 val_loss: 0.2361 val_accuracy: 0.9761
Epoch: 35 loss: 0.6041 accuracy: 0.8219 val_loss: 0.2366 val_accuracy: 0.9761
Epoch: 36 loss: 0.6200 accuracy: 0.8168 val_loss: 0.2422 val_accuracy: 0.9761
Epoch: 37 loss: 0.6133 accuracy: 0.8191 val_loss: 0.2339 val_accuracy: 0.9761
Epoch: 38 loss: 0.5839 accuracy: 0.8316 val_loss: 0.2656 val_accuracy: 0.9761
Epoch: 39 loss: 0.5797 accuracy: 0.8348 val_loss: 0.2550 val_accuracy: 0.9761
Epoch: 40 loss: 0.5698 accuracy: 0.8355 val_loss: 0.2459 val_accuracy: 0.9761
Epoch: 41 loss: 0.5657 accuracy: 0.8406 val_loss: 0.2433 val_accuracy: 0.9761
Epoch: 42 loss: 0.5752 accuracy: 0.8348 val_loss: 0.2390 val_accuracy: 0.9761

Epoch 00042: ReduceLROnPlateau reducing learning rate to 6.399999838322402e-05.
Epoch: 43 loss: 0.5763 accuracy: 0.8293 val_loss: 0.2425 val_accuracy: 0.9761
Epoch: 44 loss: 0.5550 accuracy: 0.8410 val_loss: 0.2338 val_accuracy: 0.9761
Epoch: 45 loss: 0.5715 accuracy: 0.8316 val_loss: 0.2342 val_accuracy: 0.9761
Epoch: 46 loss: 0.5711 accuracy: 0.8328 val_loss: 0.2448 val_accuracy: 0.9761
Epoch: 47 loss: 0.5830 accuracy: 0.8262 val_loss: 0.2289 val_accuracy: 0.9761
Epoch: 48 loss: 0.5646 accuracy: 0.8340 val_loss: 0.2251 val_accuracy: 0.9761
Epoch: 49 loss: 0.5682 accuracy: 0.8355 val_loss: 0.2341 val_accuracy: 0.9761
Epoch: 50 loss: 0.5659 accuracy: 0.8348 val_loss: 0.2386 val_accuracy: 0.9761
Epoch: 51 loss: 0.5661 accuracy: 0.8305 val_loss: 0.2755 val_accuracy: 0.9761
Epoch: 52 loss: 0.5541 accuracy: 0.8332 val_loss: 0.2284 val_accuracy: 0.9761
Epoch: 53 loss: 0.5459 accuracy: 0.8359 val_loss: 0.2228 val_accuracy: 0.9761
Epoch: 54 loss: 0.5626 accuracy: 0.8184 val_loss: 0.2273 val_accuracy: 0.9761
Epoch: 55 loss: 0.5444 accuracy: 0.8336 val_loss: 0.2202 val_accuracy: 0.9761
Epoch: 56 loss: 0.5491 accuracy: 0.8305 val_loss: 0.2204 val_accuracy: 0.9761
Epoch: 57 loss: 0.5527 accuracy: 0.8348 val_loss: 0.2198 val_accuracy: 0.9761
Epoch: 58 loss: 0.5714 accuracy: 0.8246 val_loss: 0.2210 val_accuracy: 0.9761
Epoch: 59 loss: 0.5579 accuracy: 0.8281 val_loss: 0.2270 val_accuracy: 0.9761
Epoch: 60 loss: 0.5740 accuracy: 0.8191 val_loss: 0.2224 val_accuracy: 0.9761
Epoch: 61 loss: 0.5398 accuracy: 0.8258 val_loss: 0.2204 val_accuracy: 0.9761
Epoch: 62 loss: 0.5163 accuracy: 0.8332 val_loss: 0.2231 val_accuracy: 0.9761

Epoch 00062: ReduceLROnPlateau reducing learning rate to 5.119999987073243e-05.
Epoch: 63 loss: 0.5569 accuracy: 0.8121 val_loss: 0.2426 val_accuracy: 0.9761
Epoch: 64 loss: 0.5305 accuracy: 0.8277 val_loss: 0.2200 val_accuracy: 0.9761
Epoch: 65 loss: 0.5285 accuracy: 0.8242 val_loss: 0.2154 val_accuracy: 0.9761
Epoch: 66 loss: 0.5123 accuracy: 0.8438 val_loss: 0.2151 val_accuracy: 0.9761
Epoch: 67 loss: 0.4902 accuracy: 0.8574 val_loss: 0.2151 val_accuracy: 0.9761
Epoch: 68 loss: 0.5044 accuracy: 0.8500 val_loss: 0.2197 val_accuracy: 0.9761
Epoch: 69 loss: 0.4872 accuracy: 0.8488 val_loss: 0.2161 val_accuracy: 0.9761
Epoch: 70 loss: 0.5137 accuracy: 0.8344 val_loss: 0.2268 val_accuracy: 0.9761
Epoch: 71 loss: 0.5030 accuracy: 0.8496 val_loss: 0.2308 val_accuracy: 0.9761

Epoch 00071: ReduceLROnPlateau reducing learning rate to 4.0960000478662555e-05.
Epoch: 72 loss: 0.4879 accuracy: 0.8605 val_loss: 0.2357 val_accuracy: 0.9761
Epoch: 73 loss: 0.4887 accuracy: 0.8566 val_loss: 0.2421 val_accuracy: 0.9761
Epoch: 74 loss: 0.4892 accuracy: 0.8527 val_loss: 0.2441 val_accuracy: 0.9761
Epoch: 75 loss: 0.5013 accuracy: 0.8520 val_loss: 0.4188 val_accuracy: 0.9239
Epoch: 76 loss: 0.5060 accuracy: 0.8461 val_loss: 0.2547 val_accuracy: 0.9717

Epoch 00076: ReduceLROnPlateau reducing learning rate to 3.2767999800853435e-05.
End of augmented training
Finish
Job ended!
