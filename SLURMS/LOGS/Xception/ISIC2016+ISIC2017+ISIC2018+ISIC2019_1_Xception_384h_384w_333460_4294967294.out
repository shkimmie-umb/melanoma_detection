Thu 09 May 2024 08:34:21 PM EDT
Python 3.9.7
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
My SLURM_ARRAY_TASK_ID: 
DB: ['ISIC2016', 'ISIC2017', 'ISIC2018', 'ISIC2019']
IMG_SIZE: [384, 384]
CLASSIFIER: Xception
JOB_INDEX: None
Start training augmented images
Combining...
Combining 1th db out of 4 dbs
Combining 2th db out of 4 dbs
Combining 3th db out of 4 dbs
Combining 4th db out of 4 dbs
Stacking data
Combining complete
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
xception (Functional)        (None, 2048)              20861480  
_________________________________________________________________
dense (Dense)                (None, 512)               1049088   
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 512)               2048      
_________________________________________________________________
dense_1 (Dense)              (None, 256)               131328    
_________________________________________________________________
batch_normalization_5 (Batch (None, 256)               1024      
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 514       
=================================================================
Total params: 22,045,482
Trainable params: 1,182,466
Non-trainable params: 20,863,016
_________________________________________________________________
Fitting ISIC2016+ISIC2017+ISIC2018+ISIC2019_aug_Xception_384h_384w_None model...
model_name: ISIC2016+ISIC2017+ISIC2018+ISIC2019_aug_Xception_384h_384w_None
Epoch: 1 loss: 0.9967 accuracy: 0.7991 val_loss: 0.9137 val_accuracy: 0.8351
Epoch: 2 loss: 0.8380 accuracy: 0.8534 val_loss: 0.8357 val_accuracy: 0.8483
Epoch: 3 loss: 0.7585 accuracy: 0.8726 val_loss: 0.7860 val_accuracy: 0.8567
Epoch: 4 loss: 0.6939 accuracy: 0.8848 val_loss: 0.7496 val_accuracy: 0.8578
Epoch: 5 loss: 0.6436 accuracy: 0.8944 val_loss: 0.7339 val_accuracy: 0.8497
Epoch: 6 loss: 0.5923 accuracy: 0.9051 val_loss: 0.7591 val_accuracy: 0.8290
Epoch: 7 loss: 0.5514 accuracy: 0.9131 val_loss: 0.6699 val_accuracy: 0.8614
Epoch: 8 loss: 0.5151 accuracy: 0.9178 val_loss: 0.6400 val_accuracy: 0.8651
Epoch: 9 loss: 0.4868 accuracy: 0.9220 val_loss: 0.5957 val_accuracy: 0.8769
Epoch: 10 loss: 0.4511 accuracy: 0.9296 val_loss: 0.6209 val_accuracy: 0.8623
Epoch: 11 loss: 0.4247 accuracy: 0.9348 val_loss: 0.5741 val_accuracy: 0.8798
Epoch: 12 loss: 0.4025 accuracy: 0.9375 val_loss: 0.5399 val_accuracy: 0.8909
Epoch: 13 loss: 0.3832 accuracy: 0.9400 val_loss: 0.5304 val_accuracy: 0.8825
Epoch: 14 loss: 0.3661 accuracy: 0.9426 val_loss: 0.5185 val_accuracy: 0.8828
Epoch: 15 loss: 0.3484 accuracy: 0.9456 val_loss: 0.5288 val_accuracy: 0.8869
Epoch: 16 loss: 0.3351 accuracy: 0.9495 val_loss: 0.5336 val_accuracy: 0.8810
Epoch: 17 loss: 0.3234 accuracy: 0.9498 val_loss: 0.5292 val_accuracy: 0.8755
Epoch: 18 loss: 0.3092 accuracy: 0.9522 val_loss: 0.4825 val_accuracy: 0.8946
Epoch: 19 loss: 0.3022 accuracy: 0.9525 val_loss: 0.4914 val_accuracy: 0.8921
Epoch: 20 loss: 0.2866 accuracy: 0.9554 val_loss: 0.5083 val_accuracy: 0.8918
Epoch: 21 loss: 0.2801 accuracy: 0.9552 val_loss: 0.4964 val_accuracy: 0.8894
Epoch: 22 loss: 0.2666 accuracy: 0.9599 val_loss: 0.4501 val_accuracy: 0.9023
Epoch: 23 loss: 0.2585 accuracy: 0.9592 val_loss: 0.4766 val_accuracy: 0.8873
Epoch: 24 loss: 0.2507 accuracy: 0.9613 val_loss: 0.4303 val_accuracy: 0.9082
Epoch: 25 loss: 0.2417 accuracy: 0.9621 val_loss: 0.4515 val_accuracy: 0.8948
Epoch: 26 loss: 0.2361 accuracy: 0.9631 val_loss: 0.4694 val_accuracy: 0.8930
Epoch: 27 loss: 0.2301 accuracy: 0.9645 val_loss: 0.4512 val_accuracy: 0.9054
Epoch: 28 loss: 0.2228 accuracy: 0.9646 val_loss: 0.4361 val_accuracy: 0.9027
Epoch: 29 loss: 0.2201 accuracy: 0.9645 val_loss: 0.4455 val_accuracy: 0.9045

Epoch 00029: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.
Epoch: 30 loss: 0.1996 accuracy: 0.9716 val_loss: 0.4123 val_accuracy: 0.9095
Epoch: 31 loss: 0.1928 accuracy: 0.9731 val_loss: 0.4576 val_accuracy: 0.8977
Epoch: 32 loss: 0.1886 accuracy: 0.9738 val_loss: 0.4049 val_accuracy: 0.9172
Epoch: 33 loss: 0.1830 accuracy: 0.9744 val_loss: 0.4173 val_accuracy: 0.9097
Epoch: 34 loss: 0.1816 accuracy: 0.9744 val_loss: 0.3877 val_accuracy: 0.9163
Epoch: 35 loss: 0.1792 accuracy: 0.9739 val_loss: 0.4186 val_accuracy: 0.9048
Epoch: 36 loss: 0.1768 accuracy: 0.9747 val_loss: 0.4102 val_accuracy: 0.9165
Epoch: 37 loss: 0.1713 accuracy: 0.9760 val_loss: 0.4224 val_accuracy: 0.9109
Epoch: 38 loss: 0.1703 accuracy: 0.9757 val_loss: 0.3980 val_accuracy: 0.9224
Epoch: 39 loss: 0.1700 accuracy: 0.9748 val_loss: 0.3861 val_accuracy: 0.9188
Epoch: 40 loss: 0.1607 accuracy: 0.9777 val_loss: 0.3787 val_accuracy: 0.9179
Epoch: 41 loss: 0.1578 accuracy: 0.9786 val_loss: 0.4268 val_accuracy: 0.9131
Epoch: 42 loss: 0.1573 accuracy: 0.9784 val_loss: 0.4150 val_accuracy: 0.9114
Epoch: 43 loss: 0.1578 accuracy: 0.9776 val_loss: 0.4138 val_accuracy: 0.9129
Epoch: 44 loss: 0.1532 accuracy: 0.9793 val_loss: 0.4006 val_accuracy: 0.9177
Epoch: 45 loss: 0.1479 accuracy: 0.9802 val_loss: 0.4061 val_accuracy: 0.9147

Epoch 00045: ReduceLROnPlateau reducing learning rate to 6.399999838322402e-05.
Epoch: 46 loss: 0.1416 accuracy: 0.9828 val_loss: 0.4221 val_accuracy: 0.9138
Epoch: 47 loss: 0.1364 accuracy: 0.9841 val_loss: 0.4089 val_accuracy: 0.9179
Epoch: 48 loss: 0.1325 accuracy: 0.9851 val_loss: 0.4149 val_accuracy: 0.9211
Epoch: 49 loss: 0.1331 accuracy: 0.9843 val_loss: 0.4000 val_accuracy: 0.9256
Epoch: 50 loss: 0.1342 accuracy: 0.9837 val_loss: 0.4030 val_accuracy: 0.9229

Epoch 00050: ReduceLROnPlateau reducing learning rate to 5.119999987073243e-05.
End of augmented training
Finish
Job ended!
