nohup: ignoring input
2024-09-30 14:20:36.532069: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-09-30 14:20:36.563287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.
  warnings.warn(
/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
  0%|          | 0/127 [00:00<?, ?it/s]  0%|          | 0/127 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "/homes/e35889/sansa/melanoma_detection/train_pytorch.py", line 192, in <module>
    model_ft = mel.Model.train_model(conf=CFG, network=model_ft, data=dataloaders, dataset_sizes=dataset_sizes)
  File "/homes/e35889/sansa/melanoma_detection/melanoma/model.py", line 88, in train_model
    outputs = network(inputs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    return self.module(*inputs[0], **module_kwargs[0])
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torchvision/models/inception.py", line 166, in forward
    x, aux = self._forward(x)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torchvision/models/inception.py", line 138, in _forward
    aux = self.AuxLogits(x)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torchvision/models/inception.py", line 386, in forward
    x = self.conv1(x)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torchvision/models/inception.py", line 405, in forward
    x = self.conv(x)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 458, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/homes/e35889/software/micromamba/envs/ood/lib/python3.10/site-packages/torch/nn/modules/conv.py", line 454, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Calculated padded input size per channel: (3 x 3). Kernel size: (5 x 5). Kernel size can't be greater than actual input size
