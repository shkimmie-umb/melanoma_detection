Wed 08 May 2024 06:45:58 PM EDT
Python 3.9.7
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:07:16_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
My SLURM_ARRAY_TASK_ID: 
DB: ['ISIC2016', 'PAD_UFES_20', 'MEDNODE', 'KaggleMB']
IMG_SIZE: [384, 384]
CLASSIFIER: EfficientNetB7
JOB_INDEX: None
Start training augmented images
Combining...
Combining 1th db out of 4 dbs
Combining 2th db out of 4 dbs
Combining 3th db out of 4 dbs
Combining 4th db out of 4 dbs
Stacking data
Combining complete
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
efficientnetb7 (Functional)  (None, 2560)              64097687  
_________________________________________________________________
dense (Dense)                (None, 512)               1311232   
_________________________________________________________________
dropout (Dropout)            (None, 512)               0         
_________________________________________________________________
batch_normalization (BatchNo (None, 512)               2048      
_________________________________________________________________
dense_1 (Dense)              (None, 256)               131328    
_________________________________________________________________
batch_normalization_1 (Batch (None, 256)               1024      
_________________________________________________________________
dense_2 (Dense)              (None, 2)                 514       
=================================================================
Total params: 65,543,833
Trainable params: 1,444,610
Non-trainable params: 64,099,223
_________________________________________________________________
Fitting ISIC2016+PAD_UFES_20+MEDNODE+KaggleMB_aug_EfficientNetB7_384h_384w_None model...
model_name: ISIC2016+PAD_UFES_20+MEDNODE+KaggleMB_aug_EfficientNetB7_384h_384w_None
Epoch: 1 loss: 1.4439 accuracy: 0.5059 val_loss: 1.1465 val_accuracy: 0.7621
Epoch: 2 loss: 1.2976 accuracy: 0.5068 val_loss: 1.1676 val_accuracy: 0.7621
Epoch: 3 loss: 1.2310 accuracy: 0.5276 val_loss: 1.1290 val_accuracy: 0.7621
Epoch: 4 loss: 1.1917 accuracy: 0.5386 val_loss: 1.0628 val_accuracy: 0.7621
Epoch: 5 loss: 1.1704 accuracy: 0.5387 val_loss: 1.1228 val_accuracy: 0.7621
Epoch: 6 loss: 1.1552 accuracy: 0.5317 val_loss: 1.0471 val_accuracy: 0.7621
Epoch: 7 loss: 1.1383 accuracy: 0.5286 val_loss: 1.0364 val_accuracy: 0.7621
Epoch: 8 loss: 1.1203 accuracy: 0.5405 val_loss: 1.0740 val_accuracy: 0.7621
Epoch: 9 loss: 1.1087 accuracy: 0.5377 val_loss: 1.0612 val_accuracy: 0.7621
Epoch: 10 loss: 1.0916 accuracy: 0.5487 val_loss: 1.0382 val_accuracy: 0.7621
Epoch: 11 loss: 1.0855 accuracy: 0.5479 val_loss: 1.0080 val_accuracy: 0.7621
Epoch: 12 loss: 1.0718 accuracy: 0.5513 val_loss: 1.0177 val_accuracy: 0.7621
Epoch: 13 loss: 1.0619 accuracy: 0.5494 val_loss: 1.0021 val_accuracy: 0.7621
Epoch: 14 loss: 1.0518 accuracy: 0.5457 val_loss: 0.9611 val_accuracy: 0.7621
Epoch: 15 loss: 1.0402 accuracy: 0.5486 val_loss: 1.0087 val_accuracy: 0.7621
Epoch: 16 loss: 1.0300 accuracy: 0.5481 val_loss: 0.8922 val_accuracy: 0.7621
Epoch: 17 loss: 1.0200 accuracy: 0.5530 val_loss: 0.9334 val_accuracy: 0.7621
Epoch: 18 loss: 1.0092 accuracy: 0.5537 val_loss: 0.9545 val_accuracy: 0.7621
Epoch: 19 loss: 0.9999 accuracy: 0.5483 val_loss: 0.9657 val_accuracy: 0.7621
Epoch: 20 loss: 0.9939 accuracy: 0.5513 val_loss: 0.8721 val_accuracy: 0.7621
Epoch: 21 loss: 0.9792 accuracy: 0.5618 val_loss: 0.8933 val_accuracy: 0.7621
Epoch: 22 loss: 0.9706 accuracy: 0.5714 val_loss: 0.8627 val_accuracy: 0.7621
Epoch: 23 loss: 0.9655 accuracy: 0.5550 val_loss: 0.9140 val_accuracy: 0.7621
Epoch: 24 loss: 0.9543 accuracy: 0.5586 val_loss: 0.8748 val_accuracy: 0.7621
Epoch: 25 loss: 0.9493 accuracy: 0.5519 val_loss: 0.8590 val_accuracy: 0.7621
Epoch: 26 loss: 0.9353 accuracy: 0.5642 val_loss: 0.9195 val_accuracy: 0.7621
Epoch: 27 loss: 0.9289 accuracy: 0.5672 val_loss: 0.8883 val_accuracy: 0.7621
Epoch: 28 loss: 0.9236 accuracy: 0.5606 val_loss: 0.8678 val_accuracy: 0.7621
Epoch: 29 loss: 0.9122 accuracy: 0.5656 val_loss: 0.8403 val_accuracy: 0.7621
Epoch: 30 loss: 0.9062 accuracy: 0.5580 val_loss: 0.8252 val_accuracy: 0.7621
Epoch: 31 loss: 0.8977 accuracy: 0.5626 val_loss: 0.8620 val_accuracy: 0.7621
Epoch: 32 loss: 0.8910 accuracy: 0.5633 val_loss: 0.7604 val_accuracy: 0.7621
Epoch: 33 loss: 0.8815 accuracy: 0.5601 val_loss: 0.8773 val_accuracy: 0.7621
Epoch: 34 loss: 0.8733 accuracy: 0.5620 val_loss: 0.8064 val_accuracy: 0.7621
Epoch: 35 loss: 0.8677 accuracy: 0.5637 val_loss: 0.8216 val_accuracy: 0.7621
Epoch: 36 loss: 0.8588 accuracy: 0.5628 val_loss: 0.8355 val_accuracy: 0.7621
Epoch: 37 loss: 0.8520 accuracy: 0.5609 val_loss: 0.7715 val_accuracy: 0.7621

Epoch 00037: ReduceLROnPlateau reducing learning rate to 7.999999797903001e-05.
Epoch: 38 loss: 0.8468 accuracy: 0.5565 val_loss: 0.7652 val_accuracy: 0.7621
Epoch: 39 loss: 0.8409 accuracy: 0.5656 val_loss: 0.8100 val_accuracy: 0.7621
Epoch: 40 loss: 0.8326 accuracy: 0.5791 val_loss: 0.7534 val_accuracy: 0.7621
Epoch: 41 loss: 0.8338 accuracy: 0.5589 val_loss: 0.7954 val_accuracy: 0.7621
Epoch: 42 loss: 0.8244 accuracy: 0.5628 val_loss: 0.7999 val_accuracy: 0.7621
Epoch: 43 loss: 0.8204 accuracy: 0.5681 val_loss: 0.7379 val_accuracy: 0.7621
Epoch: 44 loss: 0.8159 accuracy: 0.5686 val_loss: 0.7282 val_accuracy: 0.7621
Epoch: 45 loss: 0.8105 accuracy: 0.5644 val_loss: 0.7087 val_accuracy: 0.7621
Epoch: 46 loss: 0.8086 accuracy: 0.5608 val_loss: 0.7746 val_accuracy: 0.7621
Epoch: 47 loss: 0.8024 accuracy: 0.5665 val_loss: 0.7727 val_accuracy: 0.7621
Epoch: 48 loss: 0.8015 accuracy: 0.5577 val_loss: 0.7485 val_accuracy: 0.7621
Epoch: 49 loss: 0.7952 accuracy: 0.5629 val_loss: 0.6906 val_accuracy: 0.7621
Epoch: 50 loss: 0.7872 accuracy: 0.5685 val_loss: 0.7265 val_accuracy: 0.7621
Epoch: 51 loss: 0.7892 accuracy: 0.5590 val_loss: 0.6955 val_accuracy: 0.7621
Epoch: 52 loss: 0.7820 accuracy: 0.5678 val_loss: 0.6832 val_accuracy: 0.7621
Epoch: 53 loss: 0.7777 accuracy: 0.5708 val_loss: 0.7588 val_accuracy: 0.7621
Epoch: 54 loss: 0.7719 accuracy: 0.5753 val_loss: 0.7406 val_accuracy: 0.7621
Epoch: 55 loss: 0.7733 accuracy: 0.5642 val_loss: 0.6879 val_accuracy: 0.7621
Epoch: 56 loss: 0.7700 accuracy: 0.5685 val_loss: 0.7414 val_accuracy: 0.7621
Epoch: 57 loss: 0.7693 accuracy: 0.5550 val_loss: 0.7441 val_accuracy: 0.7621

Epoch 00057: ReduceLROnPlateau reducing learning rate to 6.399999838322402e-05.
Epoch: 58 loss: 0.7625 accuracy: 0.5744 val_loss: 0.7145 val_accuracy: 0.7621
Epoch: 59 loss: 0.7612 accuracy: 0.5729 val_loss: 0.6774 val_accuracy: 0.7621
Epoch: 60 loss: 0.7580 accuracy: 0.5701 val_loss: 0.6867 val_accuracy: 0.7621
Epoch: 61 loss: 0.7551 accuracy: 0.5752 val_loss: 0.6688 val_accuracy: 0.7621
Epoch: 62 loss: 0.7557 accuracy: 0.5665 val_loss: 0.6982 val_accuracy: 0.7621
Epoch: 63 loss: 0.7512 accuracy: 0.5698 val_loss: 0.6643 val_accuracy: 0.7621
Epoch: 64 loss: 0.7526 accuracy: 0.5720 val_loss: 0.6875 val_accuracy: 0.7621
Epoch: 65 loss: 0.7482 accuracy: 0.5686 val_loss: 0.6857 val_accuracy: 0.7621
Epoch: 66 loss: 0.7458 accuracy: 0.5721 val_loss: 0.6412 val_accuracy: 0.7621
Epoch: 67 loss: 0.7471 accuracy: 0.5672 val_loss: 0.6703 val_accuracy: 0.7621
Epoch: 68 loss: 0.7433 accuracy: 0.5694 val_loss: 0.6894 val_accuracy: 0.7621
Epoch: 69 loss: 0.7387 accuracy: 0.5788 val_loss: 0.6963 val_accuracy: 0.7621
Epoch: 70 loss: 0.7387 accuracy: 0.5773 val_loss: 0.6881 val_accuracy: 0.7621
Epoch: 71 loss: 0.7395 accuracy: 0.5736 val_loss: 0.6712 val_accuracy: 0.7621

Epoch 00071: ReduceLROnPlateau reducing learning rate to 5.119999987073243e-05.
Epoch: 72 loss: 0.7339 accuracy: 0.5773 val_loss: 0.6558 val_accuracy: 0.7621
Epoch: 73 loss: 0.7378 accuracy: 0.5650 val_loss: 0.6543 val_accuracy: 0.7621
Epoch: 74 loss: 0.7335 accuracy: 0.5801 val_loss: 0.6820 val_accuracy: 0.7621
Epoch: 75 loss: 0.7328 accuracy: 0.5763 val_loss: 0.6454 val_accuracy: 0.7621
Epoch: 76 loss: 0.7325 accuracy: 0.5747 val_loss: 0.6796 val_accuracy: 0.7621

Epoch 00076: ReduceLROnPlateau reducing learning rate to 4.0960000478662555e-05.
End of augmented training
Finish
Job ended!
